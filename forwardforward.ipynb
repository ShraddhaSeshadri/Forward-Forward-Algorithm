{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKlJkp3Rew-k"
      },
      "source": [
        "## Setup imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nW1PF331ew-k"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "import random\n",
        "from tensorflow.compiler.tf2xla.python import xla"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmpJAcZ1ew-l"
      },
      "source": [
        "## Load the dataset and visualize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YumBLjfCew-m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "outputId": "b2b99dc0-7ff1-4291-b143-64c49aab2ac1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 2s 0us/step\n",
            "4 Random Training samples and labels\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "print(\"4 Random Training samples and labels\")\n",
        "idx1, idx2, idx3, idx4 = random.sample(range(0, x_train.shape[0]), 4)\n",
        "\n",
        "img1 = (x_train[idx1], y_train[idx1])\n",
        "img2 = (x_train[idx2], y_train[idx2])\n",
        "img3 = (x_train[idx3], y_train[idx3])\n",
        "img4 = (x_train[idx4], y_train[idx4])\n",
        "\n",
        "imgs = [img1, img2, img3, img4]\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "for idx, item in enumerate(imgs):\n",
        "    image, label = item[0], item[1]\n",
        "    plt.subplot(2, 2, idx + 1)\n",
        "    plt.imshow(image, cmap=\"gray\")\n",
        "    plt.title(f\"Label : {label}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftIgqOqVew-n"
      },
      "source": [
        "## Define `FFDense` custom layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-m6YibXew-o"
      },
      "outputs": [],
      "source": [
        "class FFDense(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    A Dense layer that has been custom-enabled for the Forward-Forward algorithm and includes an implementation of \n",
        "    the Forward-Forward network for use. To use this layer, it must be paired with the FFNetwork model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        units,\n",
        "        optimizer,\n",
        "        loss_metric,\n",
        "        num_epochs=50,\n",
        "        use_bias=True,\n",
        "        kernel_initializer=\"glorot_uniform\",\n",
        "        bias_initializer=\"zeros\",\n",
        "        kernel_regularizer=None,\n",
        "        bias_regularizer=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = keras.layers.Dense(\n",
        "            units=units,\n",
        "            use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer,\n",
        "            bias_initializer=bias_initializer,\n",
        "            kernel_regularizer=kernel_regularizer,\n",
        "            bias_regularizer=bias_regularizer,\n",
        "        )\n",
        "        self.relu = keras.layers.ReLU()\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_metric = loss_metric\n",
        "        self.threshold = 1.5\n",
        "        self.num_epochs = num_epochs\n",
        "\n",
        "    # We perform a normalization step before we run the input through the Dense\n",
        "    # layer.\n",
        "\n",
        "    def call(self, x):\n",
        "        x_norm = tf.norm(x, ord=2, axis=1, keepdims=True)\n",
        "        x_norm = x_norm + 1e-4\n",
        "        x_dir = x / x_norm\n",
        "        res = self.dense(x_dir)\n",
        "        return self.relu(res)\n",
        "\n",
        "    '''\n",
        "    The Forward-Forward algorithm is described below: First, we perform the operation of the Dense layer \n",
        "    and then calculate the Mean Square value for both positive and negative samples. \n",
        "    Our custom loss function calculates the distance between the Mean-squared result and a threshold value, \n",
        "    which we set as a hyperparameter to determine if the prediction is positive or negative. \n",
        "    After calculating the loss, we take the mean across the entire batch and perform a gradient calculation\n",
        "    and optimization step. This is not considered traditional backpropagation, as no gradients are sent to\n",
        "    previous layers and the process is completely localized.\n",
        "    '''\n",
        "\n",
        "    def forward_forward(self, x_pos, x_neg):\n",
        "        for i in range(self.num_epochs):\n",
        "            with tf.GradientTape() as tape:\n",
        "                g_pos = tf.math.reduce_mean(tf.math.pow(self.call(x_pos), 2), 1)\n",
        "                g_neg = tf.math.reduce_mean(tf.math.pow(self.call(x_neg), 2), 1)\n",
        "\n",
        "                loss = tf.math.log(\n",
        "                    1\n",
        "                    + tf.math.exp(\n",
        "                        tf.concat([-g_pos + self.threshold, g_neg - self.threshold], 0)\n",
        "                    )\n",
        "                )\n",
        "                mean_loss = tf.cast(tf.math.reduce_mean(loss), tf.float32)\n",
        "                self.loss_metric.update_state([mean_loss])\n",
        "            gradients = tape.gradient(mean_loss, self.dense.trainable_weights)\n",
        "            self.optimizer.apply_gradients(zip(gradients, self.dense.trainable_weights))\n",
        "        return (\n",
        "            tf.stop_gradient(self.call(x_pos)),\n",
        "            tf.stop_gradient(self.call(x_neg)),\n",
        "            self.loss_metric.result(),\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz0ikLeXew-o"
      },
      "source": [
        "## Define the `FFNetwork` Custom Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeuU7CaTew-q"
      },
      "outputs": [],
      "source": [
        "\n",
        "class FFNetwork(keras.Model):\n",
        "    \"\"\"\n",
        "    A keras.Model that enables creation of a FFDense network and can be used for any type of \n",
        "    classification task. This model includes an internal implementation tailored for the \n",
        "    MNIST dataset, which can be adapted to other use cases as needed.\n",
        "    \"\"\"\n",
        "    '''\n",
        "    Since each layer performs gradient calculation and optimization locally, each layer has its \n",
        "    own optimizer. The commonly used Adam optimizer is passed with a default learning rate of \n",
        "    0.03, which was determined to be the optimal rate after experimentation. The loss is \n",
        "    monitored using the loss_var and loss_count variables.\n",
        "    '''\n",
        "    def __init__(\n",
        "        self, dims, layer_optimizer=keras.optimizers.Adam(learning_rate=0.03), **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.layer_optimizer = layer_optimizer\n",
        "        self.loss_var = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n",
        "        self.loss_count = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n",
        "        self.layer_list = [keras.Input(shape=(dims[0],))]\n",
        "        for d in range(len(dims) - 1):\n",
        "            self.layer_list += [\n",
        "                FFDense(\n",
        "                    dims[d + 1],\n",
        "                    optimizer=self.layer_optimizer,\n",
        "                    loss_metric=keras.metrics.Mean(),\n",
        "                )\n",
        "            ]\n",
        "\n",
        "    '''\n",
        "    This function dynamically modifies the image by superimposing the labels on top of it \n",
        "    (for the MNIST dataset, which has 10 unique labels, the first 10 pixels in the top-left corner are used). \n",
        "    The function returns the original data tensor with the first 10 pixels representing the labels as a \n",
        "    pixel-based one-hot representation.\n",
        "    '''\n",
        "\n",
        "    @tf.function(reduce_retracing=True)\n",
        "    def overlay_y_on_x(self, data):\n",
        "        X_sample, y_sample = data\n",
        "        max_sample = tf.reduce_max(X_sample, axis=0, keepdims=True)\n",
        "        max_sample = tf.cast(max_sample, dtype=tf.float64)\n",
        "        X_zeros = tf.zeros([10], dtype=tf.float64)\n",
        "        X_update = xla.dynamic_update_slice(X_zeros, max_sample, [y_sample])\n",
        "        X_sample = xla.dynamic_update_slice(X_sample, X_update, [0])\n",
        "        return X_sample, y_sample\n",
        "\n",
        "    \"\"\"\n",
        "    The custom function predict_one_sample makes predictions by passing images through the network, \n",
        "    evaluating the results generated by each layer (i.e. determining how high or low the output values \n",
        "    are relative to the set threshold for each label), and selecting the label with the highest values. \n",
        "    In this way, the images are evaluated for their \"goodness\" with all labels.\n",
        "    \"\"\"\n",
        "\n",
        "    @tf.function(reduce_retracing=True)\n",
        "    def predict_one_sample(self, x):\n",
        "        goodness_per_label = []\n",
        "        x = tf.reshape(x, [tf.shape(x)[0] * tf.shape(x)[1]])\n",
        "        for label in range(10):\n",
        "            h, label = self.overlay_y_on_x(data=(x, label))\n",
        "            h = tf.reshape(h, [-1, tf.shape(h)[0]])\n",
        "            goodness = []\n",
        "            for layer_idx in range(1, len(self.layer_list)):\n",
        "                layer = self.layer_list[layer_idx]\n",
        "                h = layer(h)\n",
        "                goodness += [tf.math.reduce_mean(tf.math.pow(h, 2), 1)]\n",
        "            goodness_per_label += [\n",
        "                tf.expand_dims(tf.reduce_sum(goodness, keepdims=True), 1)\n",
        "            ]\n",
        "        goodness_per_label = tf.concat(goodness_per_label, 1)\n",
        "        return tf.cast(tf.argmax(goodness_per_label, 1), tf.float64)\n",
        "\n",
        "    def predict(self, data):\n",
        "        x = data\n",
        "        preds = list()\n",
        "        preds = tf.map_fn(fn=self.predict_one_sample, elems=x)\n",
        "        return np.asarray(preds, dtype=int)\n",
        "\n",
        "    \"\"\"\n",
        "    The custom train_step function replaces the internal train_step implementation.\n",
        "     It takes all the input image tensors, flattens them, and creates positive and \n",
        "     negative samples of the images. A positive sample is an image with the correct \n",
        "     label overlayed on it using the overlay_y_on_x function, while a negative \n",
        "     sample is an image with a incorrect label. The samples are then passed through \n",
        "     each FFLayer and the Forward-Forward computation is performed. The final \n",
        "     loss value over all layers is returned as the loss.\n",
        "    \"\"\"\n",
        "\n",
        "    @tf.function(jit_compile=True)\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "\n",
        "        # Flatten op\n",
        "        x = tf.reshape(x, [-1, tf.shape(x)[1] * tf.shape(x)[2]])\n",
        "\n",
        "        x_pos, y = tf.map_fn(fn=self.overlay_y_on_x, elems=(x, y))\n",
        "\n",
        "        random_y = tf.random.shuffle(y)\n",
        "        x_neg, y = tf.map_fn(fn=self.overlay_y_on_x, elems=(x, random_y))\n",
        "\n",
        "        h_pos, h_neg = x_pos, x_neg\n",
        "\n",
        "        for idx, layer in enumerate(self.layers):\n",
        "            if isinstance(layer, FFDense):\n",
        "                print(f\"Training layer {idx+1} now : \")\n",
        "                h_pos, h_neg, loss = layer.forward_forward(h_pos, h_neg)\n",
        "                self.loss_var.assign_add(loss)\n",
        "                self.loss_count.assign_add(1.0)\n",
        "            else:\n",
        "                print(f\"Passing layer {idx+1} now : \")\n",
        "                x = layer(x)\n",
        "        mean_res = tf.math.divide(self.loss_var, self.loss_count)\n",
        "        return {\"FinalLoss\": mean_res}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIOMMRrmew-r"
      },
      "source": [
        "## Convert MNIST `NumPy` arrays to `tf.data.Dataset`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNKdxzL2ew-r"
      },
      "outputs": [],
      "source": [
        "x_train = x_train.astype(float) / 255\n",
        "x_test = x_test.astype(float) / 255\n",
        "y_train = y_train.astype(int)\n",
        "y_test = y_test.astype(int)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "\n",
        "train_dataset = train_dataset.batch(60000)\n",
        "test_dataset = test_dataset.batch(10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv-3DJLMew-r"
      },
      "source": [
        "## Fit the network and visualize results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JketzTSOew-s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1445622-e15f-4e63-860b-95dd6cbe66a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "Training layer 1 now : \n",
            "Training layer 2 now : \n",
            "Training layer 1 now : \n",
            "Training layer 2 now : \n",
            "1/1 [==============================] - 85s 85s/step - FinalLoss: 0.7278\n",
            "Epoch 2/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.7079\n",
            "Epoch 3/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.7026\n",
            "Epoch 4/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.6795\n",
            "Epoch 5/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.6559\n",
            "Epoch 6/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.6335\n",
            "Epoch 7/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.6135\n",
            "Epoch 8/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.5961\n",
            "Epoch 9/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.5806\n",
            "Epoch 10/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.5670\n",
            "Epoch 11/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.5549\n",
            "Epoch 12/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.5440\n",
            "Epoch 13/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.5340\n",
            "Epoch 14/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.5250\n",
            "Epoch 15/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.5169\n",
            "Epoch 16/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.5094\n",
            "Epoch 17/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.5025\n",
            "Epoch 18/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4962\n",
            "Epoch 19/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4903\n",
            "Epoch 20/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4847\n",
            "Epoch 21/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4795\n",
            "Epoch 22/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4746\n",
            "Epoch 23/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4700\n",
            "Epoch 24/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4657\n",
            "Epoch 25/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4617\n",
            "Epoch 26/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4578\n",
            "Epoch 27/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4542\n",
            "Epoch 28/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4507\n",
            "Epoch 29/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4473\n",
            "Epoch 30/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4442\n",
            "Epoch 31/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4411\n",
            "Epoch 32/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4382\n",
            "Epoch 33/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4355\n",
            "Epoch 34/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4328\n",
            "Epoch 35/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4303\n",
            "Epoch 36/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4279\n",
            "Epoch 37/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4256\n",
            "Epoch 38/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4234\n",
            "Epoch 39/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4212\n",
            "Epoch 40/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4191\n",
            "Epoch 41/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4171\n",
            "Epoch 42/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4152\n",
            "Epoch 43/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4133\n",
            "Epoch 44/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4115\n",
            "Epoch 45/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4097\n",
            "Epoch 46/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4080\n",
            "Epoch 47/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4064\n",
            "Epoch 48/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4048\n",
            "Epoch 49/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.4032\n",
            "Epoch 50/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4017\n",
            "Epoch 51/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4002\n",
            "Epoch 52/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3988\n",
            "Epoch 53/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3974\n",
            "Epoch 54/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3961\n",
            "Epoch 55/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3948\n",
            "Epoch 56/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3935\n",
            "Epoch 57/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3922\n",
            "Epoch 58/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3910\n",
            "Epoch 59/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3898\n",
            "Epoch 60/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3887\n",
            "Epoch 61/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3876\n",
            "Epoch 62/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3864\n",
            "Epoch 63/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3853\n",
            "Epoch 64/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3843\n",
            "Epoch 65/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3832\n",
            "Epoch 66/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3822\n",
            "Epoch 67/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3812\n",
            "Epoch 68/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3802\n",
            "Epoch 69/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3792\n",
            "Epoch 70/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3783\n",
            "Epoch 71/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3773\n",
            "Epoch 72/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3764\n",
            "Epoch 73/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3755\n",
            "Epoch 74/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3746\n",
            "Epoch 75/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3738\n",
            "Epoch 76/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3729\n",
            "Epoch 77/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3721\n",
            "Epoch 78/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3713\n",
            "Epoch 79/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3704\n",
            "Epoch 80/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3697\n",
            "Epoch 81/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3689\n",
            "Epoch 82/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3681\n",
            "Epoch 83/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3673\n",
            "Epoch 84/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3666\n",
            "Epoch 85/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3659\n",
            "Epoch 86/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3652\n",
            "Epoch 87/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3645\n",
            "Epoch 88/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3637\n",
            "Epoch 89/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3631\n",
            "Epoch 90/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3624\n",
            "Epoch 91/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3617\n",
            "Epoch 92/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3611\n",
            "Epoch 93/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3604\n",
            "Epoch 94/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3598\n",
            "Epoch 95/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3592\n",
            "Epoch 96/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3585\n",
            "Epoch 97/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3579\n",
            "Epoch 98/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3573\n",
            "Epoch 99/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3567\n",
            "Epoch 100/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3561\n",
            "Epoch 101/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3555\n",
            "Epoch 102/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3550\n",
            "Epoch 103/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3544\n",
            "Epoch 104/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3538\n",
            "Epoch 105/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3532\n",
            "Epoch 106/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3527\n",
            "Epoch 107/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3522\n",
            "Epoch 108/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3516\n",
            "Epoch 109/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3511\n",
            "Epoch 110/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3506\n",
            "Epoch 111/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3500\n",
            "Epoch 112/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3496\n",
            "Epoch 113/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3491\n",
            "Epoch 114/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3486\n",
            "Epoch 115/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3481\n",
            "Epoch 116/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3476\n",
            "Epoch 117/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3471\n",
            "Epoch 118/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3466\n",
            "Epoch 119/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3462\n",
            "Epoch 120/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3457\n",
            "Epoch 121/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3452\n",
            "Epoch 122/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3448\n",
            "Epoch 123/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3443\n",
            "Epoch 124/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3439\n",
            "Epoch 125/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3435\n",
            "Epoch 126/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3430\n",
            "Epoch 127/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3426\n",
            "Epoch 128/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3422\n",
            "Epoch 129/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3418\n",
            "Epoch 130/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3414\n",
            "Epoch 131/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3410\n",
            "Epoch 132/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3405\n",
            "Epoch 133/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3402\n",
            "Epoch 134/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3398\n",
            "Epoch 135/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3394\n",
            "Epoch 136/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3390\n",
            "Epoch 137/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3386\n",
            "Epoch 138/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3383\n",
            "Epoch 139/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3379\n",
            "Epoch 140/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3375\n",
            "Epoch 141/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3371\n",
            "Epoch 142/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3368\n",
            "Epoch 143/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3364\n",
            "Epoch 144/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3361\n",
            "Epoch 145/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3357\n",
            "Epoch 146/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3354\n",
            "Epoch 147/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3350\n",
            "Epoch 148/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3347\n",
            "Epoch 149/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3343\n",
            "Epoch 150/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3340\n",
            "Epoch 151/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3337\n",
            "Epoch 152/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3334\n",
            "Epoch 153/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3330\n",
            "Epoch 154/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3327\n",
            "Epoch 155/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3324\n",
            "Epoch 156/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3321\n",
            "Epoch 157/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3318\n",
            "Epoch 158/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3315\n",
            "Epoch 159/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3312\n",
            "Epoch 160/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3309\n",
            "Epoch 161/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3306\n",
            "Epoch 162/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3303\n",
            "Epoch 163/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3300\n",
            "Epoch 164/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3297\n",
            "Epoch 165/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3294\n",
            "Epoch 166/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3291\n",
            "Epoch 167/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3289\n",
            "Epoch 168/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3286\n",
            "Epoch 169/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3283\n",
            "Epoch 170/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3280\n",
            "Epoch 171/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3277\n",
            "Epoch 172/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3275\n",
            "Epoch 173/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3272\n",
            "Epoch 174/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3269\n",
            "Epoch 175/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3267\n",
            "Epoch 176/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3264\n",
            "Epoch 177/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3261\n",
            "Epoch 178/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3259\n",
            "Epoch 179/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3257\n",
            "Epoch 180/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3254\n",
            "Epoch 181/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3251\n",
            "Epoch 182/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3249\n",
            "Epoch 183/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3246\n",
            "Epoch 184/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3244\n",
            "Epoch 185/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3242\n",
            "Epoch 186/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3239\n",
            "Epoch 187/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3237\n",
            "Epoch 188/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3235\n",
            "Epoch 189/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3232\n",
            "Epoch 190/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3230\n",
            "Epoch 191/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3228\n",
            "Epoch 192/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3226\n",
            "Epoch 193/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3223\n",
            "Epoch 194/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3221\n",
            "Epoch 195/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3219\n",
            "Epoch 196/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3217\n",
            "Epoch 197/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3215\n",
            "Epoch 198/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3213\n",
            "Epoch 199/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3210\n",
            "Epoch 200/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3208\n",
            "Epoch 201/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3206\n",
            "Epoch 202/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3204\n",
            "Epoch 203/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3202\n",
            "Epoch 204/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3200\n",
            "Epoch 205/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3198\n",
            "Epoch 206/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3196\n",
            "Epoch 207/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3194\n",
            "Epoch 208/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3192\n",
            "Epoch 209/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3190\n",
            "Epoch 210/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3188\n",
            "Epoch 211/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3186\n",
            "Epoch 212/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3184\n",
            "Epoch 213/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3182\n",
            "Epoch 214/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3180\n",
            "Epoch 215/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3178\n",
            "Epoch 216/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3177\n",
            "Epoch 217/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3175\n",
            "Epoch 218/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3173\n",
            "Epoch 219/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3171\n",
            "Epoch 220/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3169\n",
            "Epoch 221/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3167\n",
            "Epoch 222/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3165\n",
            "Epoch 223/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3163\n",
            "Epoch 224/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3162\n",
            "Epoch 225/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3160\n",
            "Epoch 226/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3158\n",
            "Epoch 227/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3157\n",
            "Epoch 228/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3155\n",
            "Epoch 229/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3153\n",
            "Epoch 230/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3151\n",
            "Epoch 231/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3150\n",
            "Epoch 232/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3148\n",
            "Epoch 233/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3146\n",
            "Epoch 234/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3145\n",
            "Epoch 235/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3143\n",
            "Epoch 236/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3142\n",
            "Epoch 237/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3140\n",
            "Epoch 238/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3138\n",
            "Epoch 239/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3137\n",
            "Epoch 240/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3135\n",
            "Epoch 241/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3134\n",
            "Epoch 242/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3132\n",
            "Epoch 243/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3131\n",
            "Epoch 244/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3129\n",
            "Epoch 245/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3128\n",
            "Epoch 246/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3126\n",
            "Epoch 247/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3125\n",
            "Epoch 248/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3123\n",
            "Epoch 249/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3121\n",
            "Epoch 250/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3120\n"
          ]
        }
      ],
      "source": [
        "model = FFNetwork(dims=[784, 500, 500])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.03),\n",
        "    loss=\"mse\",\n",
        "    jit_compile=True,\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "epochs = 250\n",
        "history = model.fit(train_dataset, epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKVoU2ZSew-t"
      },
      "source": [
        "## Perform inference and testing\n",
        "\n",
        "Having trained the model to a large extent, we now see how it performs on the\n",
        "test set. We calculate the Accuracy Score to understand the results closely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfNBgnpEew-t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8efdf75-4fb2-4e1e-8c08-0a59338711f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy score : 97.77%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "preds = model.predict(tf.convert_to_tensor(x_test))\n",
        "\n",
        "preds = preds.reshape((preds.shape[0], preds.shape[1]))\n",
        "\n",
        "results = accuracy_score(preds, y_test)\n",
        "\n",
        "print(f\"Test Accuracy score : {results*100}%\")\n",
        "\n",
        "plt.plot(range(len(history.history[\"FinalLoss\"])), history.history[\"FinalLoss\"])\n",
        "plt.title(\"Loss over training\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kEe28lUew-u"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This example has hereby demonstrated how the Forward-Forward algorithm works using\n",
        "the TensorFlow and Keras packages. While the investigation results presented by Prof. Hinton\n",
        "in their paper are currently still limited to smaller models and datasets like MNIST and\n",
        "Fashion-MNIST, subsequent results on larger models like LLMs are expected in future\n",
        "papers.\n",
        "\n",
        "Through the paper, Prof. Hinton has reported results of 1.36% test accuracy error with a\n",
        "2000-units, 4 hidden-layer, fully-connected network run over 60 epochs (while mentioning\n",
        "that backpropagation takes only 20 epochs to achieve similar performance). Another run of\n",
        "doubling the learning rate and training for 40 epochs yields a slightly worse error rate\n",
        "of 1.46%\n",
        "\n",
        "The current example does not yield state-of-the-art results. But with proper tuning of\n",
        "the Learning Rate, model architecture (number of units in `Dense` layers, kernel\n",
        "activations, initializations, regularization etc.), the results can be improved\n",
        "to match the claims of the paper."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8oD1F6THchHR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}